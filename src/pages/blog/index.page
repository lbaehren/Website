---
title: "Blog"
in_menu: false
author: "Lars Baehren"
---

# {title:} #

**:::**

### Knapp ###


_Thu, 22. January 2015 -- 17:11_

Ich sollte es eigentlich besser wissen: um diese Tageszeit versuchen mit der Linie
12 vom Uithof zum Bahnhof zu gelangen ist ein reines Gluecksspiel. Dass die Strassen
zwischen Uithof und der Autobahnauffahr vollkommen verstopft sind waere das eine
-- wenn der Bus aber eh schon mehrere Minuten Verspaetung mit bringt, dann ist die
Wahrscheinlichkeit wieder gut zu machen noch einmal eine Spur geringer. So kam es dann
auch, dass ich gerade einmal den Fuss auf den Bahnsteg gesetzt hatte, als der ICE
auch schon einfuhr. Fuer die Zukunft unbedingt merken: selbst wenn auf den Anzeigen
die Linie 28/128 spaeter angezeigt wird als die Linie 12, ist es immer noch sinnvoller
den Weg westlich am Zentrum vorbei zu nehmen, da die Busse hier ueber weite Strecken
ihre eigene Fahrspur haben und ihr Fortkommen demnach von dem des restlichen Autoverkehr
entkoppelt ist. Jetzt muss ich nur noch daran denken, die eigenen Ratschlaege auch
selber zu beruecksichtigen...

**:::**

### Omegatau (164) Satellite-based Earth Observation ###


_Wed, 21. January 2015 -- 22:20_

Es war im September letzten Jahres, wo ich mich -- nach dem Anhoeren einer weiteren
Episode -- entschlossen habe mit [Omegatau](http://omegataupodcast.net) Kontakt aufzunehmen

> Hallo Markus,
> Hallo Nora,
>
> im Anschluss an die (sehr schoene) [Folge mit RadioMono](http://www.radiomono.net/?podcast=rmn-013-omegataupodcastbesuch) war ich schon ein klein wenig in mich gegangen, um einmal zu schauen, in welcher Weise ich Euch noch bei der Suche nach Themen und Gespraechspartnern behilflich sein kann. Gerade die Bemerkung, dass es derzeit ein Shortage an englischen Sendungen geben wuerde hat mich aufmerksam werden lassen. Als ich dann in der nun aktuellen Folge ueber Photogrammetrie (uebrigends sehr schoen, dass Ihr noch diese Schreibung verwendet) Markus Wunsch nach Folgen ueber satellitengestuetzte Beobachtungen hoerte, wollte ich mal eine Variante unterbreiten, ueber welche sich beide Aspekte zusammenbringen lassen.

Mein Aufhaenger Stelle war, dass [Episode 155 – Photogrammetrie im Himalaya](http://omegataupodcast.net/2014/09/155-photogrammetrie-im-himalaya-mwp-teil-2)
eigentlich nach einem Gegenstueck verlangte, bei welchem man den Blick zurueck auf die
Erde aus einer noch einmal deutlich groesseren Entfernung wirft, als dies bei einem
Flugzeug der ist. Und da Markus von sich auch schon anmerkte, dass es eigentlich ganz
gut waere eine Episode ueber Satelliten-gestuetzte Erdbeobachtungen zu machen, ging bei
mir direkt ein Licht an. Da ich durch meine Arbeit bei [SRON](https://www.sron.nl)
ja direkt an der Quelle sitze, schien es einfach logisch zu versuchen die Dinge hier
zusammenzufuehren. Bis es allerdings von der ersten Idee zur wirklichen Aufnahme der
Folge (vor Ort im Institut) kam, gab es doch noch einiges Hin und Her -- vor allen Dingen
das Auffinden von Gespraechspartner gestaltete sich ein wenig muehsam, so dass ich Ruud
sehr dankbar bin, dass er sich fuer das Vorhaben begeistern und freimachen konnte.

![Screenshot website](/blog/2015/01/2015-01-18_omegatau_164.png)

Am vergangenen Wochenende dann ist [Episode 164](http://omegataupodcast.net/2015/01/164-satellite-based-earth-observation)
dann erschienen und ich bin (wie sich bestimmt nachvollziehen laesst) sehr gespannt gewesen.
Dass ich die etwas mehr als zwei Stunden Hoermaterial noch vor der Abfahrt am Montag
durchbekommen habe, ist vor allen Dingen der Tatsache geschuldet, dass ich am Sonntag
mit Sofia auf einem sehr langen Spaziergang unterwegs war; so konnte ich nicht nur dem
Hund (und natuerlich auch mir selber) einiges an Bewegung an der frischen Luft verschaffen,
sondern auch die komplette Folge von Anfang bis Ende durchhoeren. Das Ergebnis ist sehr schoen
geworden und ich habe noch einiges hinzu lernen koennen. Wer also ein wenig mehr von
dem erfahren will, woran bei SRON gearbeitet wird (und womit ich ja auch zum Teil meine
Zeit verbringe), dem kann ich diese Folge waermstens empfehlen. Beobachtung und
Versehen der Ablaeufe auf unserem blauen Planeten sind eine Sache, deren Bedeutung man
sicherlich kaum unterschaetzen kann -- hierzu einen kleinen Beitrag liefern zu koennen
ist schon etwas, 

**:::**

### How much storage space are you using?﻿ ###


_Tue, 20. January 2015 -- 22:19_

[Drobo](https://plus.google.com/u/0/+Drobo) a short while ago posted this on Google+:

> Those hard drives stack up fast! How much storage space are you using?﻿

Attached to the question was a small poll I could not resist to answer:

![Screenshot from Google+](/blog/2015/01/2015-01-20_Storage_space.png)

I have to say, the numbers themselves unfortunately are not that interesting -- the
the information I would really find more useful is how that disk space is being set
up and used.

**:::**

### Index Seiten ###


_Mon, 19. January 2015 -- 22:49_

So, ich denke mal, dass ich jetzt endlich auch die letzten der bisher noch fehlenden
Index-Seiten angelegt kriege: nachdem die Erzeugung der Uebersichtsseiten fuer die
Blog-Eintraege eines Jahres ja implementiert war, habe ich mir gerade eben mal noch
ein paar Minuten geschnappt, um die fehlenden Zeilen Bash Script zu schreiben. Viel
fehlte nicht mehr:

~~~~ bash
    # page header segment
    echo "---"                                               > ${varIndexMonth}
    echo "title: \"${varMonthName} ${varYear}\""            >> ${varIndexMonth}
    echo "in_menu: false"                                   >> ${varIndexMonth}
    echo "author: \"Lars Baehren\""                         >> ${varIndexMonth}
    echo "---"                                              >> ${varIndexMonth}
    echo ""                                                 >> ${varIndexMonth}

    # loop over entries during the month
    echo " * [${varEntry}](${varLink}) \| ${varTimeheader}" >> ${varIndexMonth}
~~~~

Waehrend ich damit alle wesentlichen Seiten erzeugt haben sollte, bleibt natuerlich
immer noch das Problem, dass ich es mittlerweile mit so vielen Seiten zu tun habe,
dass die blosse Generierung aus den Quelldateien locker 10 Minuten brauchen kann.
Ich denke mal, dass dies mit Sicherheit einer der Punkte ist, welchen ich mir mal
noch genauer unter die Lupe nehmen muss; grundsaetzlich moechte ich schon die
Verwendung statischer Seiten beibehalten (es sei denn es gibt einen wirklich guten
Grund davon abzuweichen), nur letzten Endes habe ich dann mit einem Skalierungsproblem
zu tun.

**:::**

### Ubuntu: Download updates while installing ###


_Mon, 19. January 2015 -- 14:12_

As far as I am concerned I have not seen any (desktop) version of [Ubuntu](http://www.ubuntu.com)
where this feature works properly:

![Screenshot from install dialog](/blog/2015/01/2015-01-19_Ubuntu_install.png)

My expectation would be that when afterwards rebooting into a newly installed system
running

~~~~ bash
    apt-get update
    apt-get upgrade
~~~~

should not lead to any further package updates. The contrary is the case: even
before I manage to open up a terminal window to type in the above commands, the
"Software Updater" pops up to announce there are several updates available -- not
the kind of behaviour I would expect when checking the corresponding box earlier in
the process.

**:::**

### Wo der Doppelpunkt sitzt ###


_Thu, 15. January 2015 -- 17:55_

Geschafft! Das hat mich jetzt zwar ein wenig Zeit gekostet, aber nach laengerem Suchen
und testen habe ich endlich den Fehler festmachen (und auch beheben) koennen, welcher
dazu fuehrte, dass ein Teil der in den Algorithmen berechneten Ergebnisse falsch weggeschrieben
wurden (und ich meine damit nicht nur meinen Code, sondern innerhalb des gesamten Frameworks).
Worauf es letzten Endes hinauslief war ein recht subtiler Tipfehler in dem Verweis
auf den Teil eines Arrays: statt

~~~~ python
    b = a[:, :nofColumns]
~~~~

stand in der entsprechenden Routine ein

~~~~ python
    b = a[:, nofColumns:]
~~~~

was natuerliche keinesfalls das Gleiche ist. Um dies zu verdeutlichen (schliesslich
kann ich an dieser Stelle dann doch nicht die gleiche Gelaeufigkeit mit entsprechenden
Code-Schnippseln voraussetzen) hier mal ein ganz einfaches Beispiel: wenn ich mit einem
ganz einfachen linearen Array starte

~~~~
    a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
~~~~

kriege ich -- je nachdem welche der beiden zuvor erwaehnten Schreibweissen ich verwende --
die folgenden Ergebnisse:

~~~~
    a[:5]  =>  [0, 1, 2, 3, 4]

    a[5:]  =>  [5, 6, 7, 8, 9]
~~~~

Wie sich also erkennen laesst macht schon einen Unterschied, wie die relative Position
von Index-Nummer und Doppelpunkt ist. Damit wird dann (hoffentlich) auch verstaendlich
was fuer eine Auswirkung es haben kann, wenn man statt

~~~~ python
    b_left  = a[:nofColumns]    #  [0, 1, 2, 3, 4]
    b_right = a[nofColumns:]    #  [5, 6, 7, 8, 9]
~~~~

faelschlicherweise das folgende im Code stehen hat:

~~~~ python
    b_left  = a[:nofColumns]    #  [0, 1, 2, 3, 4]
    b_right = a[:nofColumns]    #  [0, 1, 2, 3, 4]
~~~~

Richtig, in `b_left` und `b_right` stehen die gleichen Werte. Das ganze ist durchaus
subtil genug um recht lange unbemerkt zu bleiben... es sei denn man schaut hier mal
wirklich genau hin (z.B. durch das Schreiben entsprechender Testroutinen). Genau auf
diesem Wege bin ich dem Fehler auch auf die Schliche gekommen, weil ich naemlich
(nur der Sicherheit halber) verifizieren wollte, dass die Daten, welche ich auf die
Platte wegschreibe und dann wieder einlese auch entsprechen womit ich gestartet bin.
Wie sich dabei herausstellte war eben genau dies nicht der Fall. Da ich mir da zunaechst
einmal keinen Reim draus machen konnte habe ich einfach ein wenig weitergegraben, bis
ich mir die erzeugten Dateien noch einmal auf anderem Wege genauer angeschaut habe:
dank dem Schema der erzeugten (und weggeschriebenen) Daten trat der Fehler dabei
recht deutlich in Erscheinung, so dass es jetzt nur noch eine Frage war, wo sich
der entsprechende Dreher denn finden lassen wuerde. Dass ich dabei in einem recht
grundlegenden Module des Frameworks fuendig geworden bin, hat schon etwas befriedigendes:
dies bedeutet naemlich nicht nur, dass der Fehler nicht auf meiner Seite lag, sondern
dass ich mit der Korrektur automatisch diesen Fehler fuer alle anderen Benutzer mitbehoben
habe. Interessant, dass dis bisher anscheinend noch niemandem aufgefallen ist, denn
dies zieht sich dann letzten Endes durch alle Berechnungen hindurch!

**:::**

### Zeit nehmen zum Lesen ###


_Tue, 13. January 2015 -- 23:00_

Es gibt einfach Dinge, fuer welche man sich bewusst Zeit nehmen muss -- Dinge, welche
nicht einfach so nebenher, parallel passieren, sondern eine gewisse Mindestkonzentration
erfordern. Nachrichten, Reportagen, Interviews, etc, anhoeren, das alles laesst sich
wunderbar mit den diversesten Taetigkeiten kombinieren -- ein Buch lesen aber vertraegt
sich nur mit einer deutlich kleineren Anzahl von anderen Aktivitaeten. Von diesen
wenigen gibt es zwar einige besonders angenehme (wie zum Beispiel eine Massage geben),
aber bei wenig Licht im Regen durch die Gegend laufen gehoert nicht unbedingt zu den
Umstaenden, welche sich zum Lesen eignen.

Typischerweise scheint es bei mir mit dem Lesen immer ein wenig phasenweise zu kommen:
ueber einen gewissen Zeitraum bleibe ich konstant dabei und schaffe es so ein Buch
nach dem anderen "abzuarbeiten" -- dann tritt wieder eine Phase ein, wo meine Leseliste
keinerlei Anstalten macht kuerzer zu werden. Gut, von alleine passiert letzteres natuerlich
nicht, da gehoert auch schon ein wenig Selbstdisziplin dazu, so dass ich seit dem Ende
letzten Jahres schon versuche wieder kontinuierlicher dran zu bleiben. Da macht es dann
auch Sinn nicht einfach nur eine Liste abzuarbeiten (fuer die derzeit aeltesten Eintraege
siehe unten), sondern sich eher an der Interessenlage zu orientieren -- so bedeutet
das Lesen keine Anstrengung sondern Genuss und Bereicherung.

~~~~
ID Proj  Age  Description
-- ----- ---- ------------------------------------------
 3 books 3.5y Heretics of Dune (Frank Herbert)
 4 books 3.5y Chapterhouse Dune (Frank Herbert)
 5 books 3.5y The Virgin Suicides (Jeffrey Eugenides)
 6 books 3.5y Chronicles of Narnia (C.S. Lewis)
 7 books 3.5y White Orleander (Janet Fitch)
 8 books 3.5y Picture perfect (Jodi Picault)
 9 books 3.5y Nineteen minutes (Jodi Picault)
10 books 3.5y Perestroika (Michail Gorbatschow)
11 books 3.5y Die Entdeckung des Himmels (Harry Mulisch)
12 books 3.5y The Audacity of Hope (Barack Obama)
~~~~

Zwischen Weihnachten und Neujahr habe ich mit "[Bilder im Kopf: Die Geschichte meines
Lebens](http://www.amazon.de/Bilder-Kopf-Geschichte-meines-Lebens/dp/3421045666)" begonnen,
der Autobiographie von [Michael Ballhaus](http://de.wikipedia.org/wiki/Michael_Ballhaus).
Wer so auf Anhieb nichts mit dem Namen anfangen kann (Schande, Schande, Schande),
der sollte mal einen Blick auf das [IMDb Profil](http://www.imdb.com/name/nm0000841)
werfen. Auch wenn ich selbst nicht unbedingt die vollstaendige Liste der Filme, an welchen
der Mann mitgewirkt hat, aufzaehlen koennte, so sind mir vor allen Dingen die Streifen
gelaeufig, welche aufgrund ihrer Kamera-Arbeit -- oder einer Erzaehlung, welche der Zeit
voraus ist -- besonders bemerkenswert sind:

 - [Welt am Draht](http://www.imdb.com/title/tt0070904) (1973)
 - [The Color of Money](http://www.imdb.com/title/tt0090863) (1986)
 - [The Last Temptation of Christ](http://www.imdb.com/title/tt0095497) (1988)
 - [The Fabulous Baker Boys](http://www.imdb.com/title/tt0097322) (1989)
 - [Goodfellas](http://www.imdb.com/title/tt0099685) (1990)
 - [The Legend of Bagger Vance](http://www.imdb.com/title/tt0146984) (2000)

... um zumindest mal ein paar zu nennen. In der [ARD Mediathek](http://www.ardmediathek.de)
gibt es uebrigens auch einen [Beitrag](http://www.ardmediathek.de/tv/ttt-titel-thesen-temperamente/Michael-Ballhaus-Bilder-im-Kopf/Das-Erste/Video?documentId=20239456&bcastId=431902)
anlaesslich der Buchveroeffentlichung -- ein recht nettes, kurzes Portrait, aber wer
mehr will, dem sei das Buch waermstens ans Herz gelegt.

Was war jetzt aber der eigentliche Aufhaenger? Als ich "Bilder im Kopf" am Sonntag Abend
zugemacht habe, hatte ich noch weniger als 100 Seiten zu lesen; da scheint es mir doch
realistisch, dass ich diesen verbleibenden Rest noch vor dem Ende der Woche -- vielleicht
aber auch schon vor dem Donnerstag Abend -- schaffe. Motivation habe ich in jedem Fall, denn
a) ist das Buch wirklich sehr interessant und trotz sein gelegentlichen sprachlichen
Eigenheiten gut lesbar und b) will ich unbedingt mit "[The Last Ship](http://en.wikipedia.org/wiki/The_Last_Ship_%28novel%29)"
beginnen.

**:::**

### Ein wenig ruhiger ###


_Mon, 12. January 2015 -- 10:01_

Normalerweise benutze ich ja dir Fahrt von Bonn nach Utrecht (und natuerlich auch in
die andere Richtung) dazu noch ein wenig zu arbeiten und ggf. Dinge erledigen, welche
ich dann (einmal angekommen) z.B. abschicken kann. Was dies allerdings ein wenig
voraussetzt ist, dass es mit der Konzentration klappt -- bei der Erkaeltung, welche
mir immer noch in den Knochen sitzt war es eben darum nicht so sonderlich gestellt.
Folgerichtig habe ich die Fahrt deutlich ruhiger angehen lassen und lieber noch
ein bischen Schlaf nachgeholt, damit ich einigermassen erholt im Institut ankomme.

**:::**

### Tannenbusch in HDR ###


_Fri, 09. January 2015 -- 19:02_

Sieht ganz danach aus als wuerde ich langsam das Bildmaterial fuer eine kleine Sammlung
an [Tannenbusch-Impressionen](https://www.flickr.com/photos/larsbaehren/sets/72157645059320272)
zusammenkriegen. Nachdem ich ein wenig enttaeuscht war von Fotos, welche ich online
finden konnte, habe ich mir vorgenommen Tannenbusch auch einmal von der fotographisch
vorteilhafteren Seite zu praesentieren. Ein klein wenig habe ich damit ja schon im
letzten Jahr begonnen, aber ich denke mal 2015 wird ausreichend Gelegenheiten bieten
da noch so das eine oder andere Foto hinzuzufuegen.

![Haltestelle Tannenbusch Sued](/blog/2015/2015-01/hdr_20150104_174853_Tannenbusch_Sued.jpg)

Obiges Foto ist vom letzten Sonntag, als ich mir die Kamera geschnappt habe um einfach
mal eine Stunde lang durch die Gegend zu laufen und nach Motiven zu suchen. Fuendig
geworden bin ich dabei unter anderen bei der Haltestelle "Tannenbusch Sued" der Bahnlinien
16 und 63 (ueber welche es eine schnelle Anbindung zum Hauptbahnhof gibt). Dass sich
mit der Szene etwas interessantes wuerde machen lassen war mir eigentlich direkt klar
-- nur bei der Nachbearbeitung nachher am Rechner musste ich ein wenig experimentieren,
denn die Lampen ueber der Bruecke verursachen eine ziemlich starke Verfaerbung von allem
was sichim Vordergrund befindet. Worauf ich mich dann schliesslich festgelegt habe ist,
dass ich versuche die Betonwaende auf einen neutralen Ton zu setzen, damit ich dieses
orange Schimmern wegkriege -- der nette Nebeneffekt davon ist aber, dass sowohl das Metall
der Aufzuege als auch der Himmel noch einmal eine ganze Ecke kuehler wirken (weil eben
die Rotanteile herausgefiltert werden).

**:::**

### ESA, nicht NASA ###


_Thu, 08. January 2015 -- 18:19_

Nachdem ich letztes Jahr schon wieder bei [Omegatau](http://omegataupodcast.net)
und [FLOSS Weekly](http://twit.tv/show/floss-weekly) aktiv geworden bin, was die
Rueckmeldung und Mithilfe betrifft, so war es um den Jahreswechsel auch mal an der
Zeit mich mit der [BBC](http://www.bbc.co.uk) in Verbindung zu setzen. Grund dafuer
war ein -- wie ich fand einfach zu prominenter sachlicher Fehler -- in der letzten
Ausgabe des [Global News Podcast](http://www.bbc.co.uk/podcasts/series/globalnews)
vor dem Jahreswechsel:

> Sent: 02 January 2015 21:24  <br>
> To: Global Podcast  <br>
> Subject: Factual error in 31-Dec-2014 edition
>
>
> Ladies and Gentlemen,
>
>
> the last edition of the Global News Podcast for the year 2014 contained
> a prominently features factual error:
>
>
> The Rosetta mission is not run by [NASA](http://www.nasa.gov) - as falsely claim in the
> introduction to the subsequent report - by the [European Space Agency](http://www.esa.int/ESA)
> (ESA). Given that in fact this mission was probably THE scientific
> highlight (especially considering media impact) of 2014 the attribution
> of the organization responsible should be correct.  <br>
>
> For verification check: [rosetta.esa.int](http://rosetta.esa.int)

Was es sonst vielleicht noch alles an Ausrutschern gibt kann ich nur schwer beurteilen,
aber es gibt einfach Geschichten wie diese, wo ich ziemlich nah an der Quelle sitze
(zumindest deutlich naeher als der durchschnittliche Hoerer); wenn man mir dann versucht
zu erzaehlen dass _Rosetta_ eine amerikanische Raumfahrt-Mission ist -- wenn alle
Live-Bildern von der Landung doch aus dem Kontrollzentrum in Deutschland kamen -- dann
wird ein Einspruch faellig. Dem Feedback nach, welches ich dann Beginn dieser Woche
erhalten habe, war ich wohl nicht der einzige, der auf den Fehler hingewiesen hat
(waere auch wirklich verwunderlich gewesen nach all dem Medien-Zirkus Ende letzten
Jahres):

> Thanks for pointing this out, and you're not the only listener to have spotted it. I was the producer on duty that night, and I take responsibility for the mistake - it was a bad one and shouldn't have occurred.   We'll be more vigilant in future.
>
> Rahul Sarnaik,  <br>
> Producer,  <br>
> GLOBAL NEWS,  <br>
> BBC World Service.

Naja, ich werde einfach mal die Ohren offen halten...

**:::**

### Deutlich schneller ###


_Wed, 07. January 2015 -- 20:10_

Es gibt durchaus schon einen guten Grund dafuer, dass ich mir ab und zu einmal groessere
Berge an Papier von Bonn mit nach Utrecht nehme. Ist ist schon ein Unterschied, ob ich
es mit einem hausueblichen Drucker/Scanner zu tun habe, oder aber mit einer jener
High-Tech Geraete wie sie hier vor Ort zur Verfuegung stehen: Stapel Papier einlegen,
Email-Adresse eingeben, noch ein paar Einstellungen anpassen und schon geht es ab.
Gut, wenn es man so an die 100 Seiten auflegt, dauert es auch hier schon mal eine kleine
Weile, aber a) ist diese Weile deutlich kuerzer und b) ist die resultierende Qualitaet
noch einmal eine ganze Ecke besser. Aber seien wir mal realistisch: fuer den heimischen
Gebrauch waere so ein Druck-Center vollkommener Overkill. Ist aber dennoch ganz nett,
wenn man ab und zu darauf Zugriff hat, wenn es darum geht einen grossen Stapel Papier
in digitaler Form in den Rechner zu kriegen.

**:::**

### Abschied von der Mittelwelle ###


_Tue, 06. January 2015 -- 18:13_

Wahrscheinlich sind es wirklich nur noch Nostalgiker, welche ueberhaupt die entsprechenden
Empfangsgeraete haben, mit welchen sich ueber Mittelwelle ausgestrahle Nachrichten
belauschen liessen. Konsequenterweise stellen die Radiosender dieses Angebot ein:

> Fast alle öffentlich-rechtlichen Radiosender in Deutschland haben den Empfang über Mittelwelle (MW) abgeschafft. Nachdem Südwestrundfunk, Mitteldeutscher Rundfunk, Hessischer Rundfunk, Rundfunk Berlin-Brandenburg, Deutschlandradio Kultur und Radio Bremen dieses Angebot beendet haben, wird auch der Norddeutsche Rundfunk (NDR) von Montag an keine Sendungen mehr über MW ausstrahlen.
>
> \[...\]
>
> Die Mittelwelle deckt im Hörfunk den Frequenzbereich ungefähr zwischen 530 Kilohertz und 1600 Kilohertz ab. Die Technik spielte beim Aufbau des Radios in Deutschland von den 20er Jahren noch bis in die 50er eine große Rolle. Seit der Nachkriegszeit verdrängte aber die Ultrakurzwelle (UKW) beim breiten Publikum diese Technik weitgehend. Trotz des knarzigen Empfangs wurde die MW aber noch jahrzehntelang von vielen Hörern wegen ihrer großen Reichweite geschätzt. Seit dem Aufkommen des mobilen Internets und der Digitalradio-Geräte gilt die Mittelwelle als Technik von gestern.

[Wie Heise News weiter berichten](http://www.heise.de/newsticker/meldung/Fast-alle-ARD-Radiosender-stellen-Mittelwelle-ein-2512316.html)
steht aber auch die Sendung via UKW auf der potentiellen Abschussliste:

> Auch UKW wird immer wieder hinterfragt. Deutschlandradio-Intendant Willi Steul zum Beispiel hat sich darüber hinaus wiederholt für eine Abschaltung auch von UKW starkgemacht. Ursprünglich sollten schon bis zum Jahr 2010 alle Radiosender ihren Sendebetrieb auf das digitale DAB (Digital Audio Broadcasting) umgestellt haben. Nach massiven Widerständen hatte der Gesetzgeber einen festen Termin gestrichen.

Die Anzahl der hier vorhandenen Empfangsgeraete duerfte nach sicherlich noch groesser
Null sein, aber grossartige Verkaufszahlen in dem Segment kann ich mir nur schwerlich
vorstellen. Ist wahrscheinlich wirklich nur eine Frage der Zeit, bis auch hier das Licht
ausgeht (auch wenn dies nicht heisst, dass damit automatisch das zugehoerige Frequenzband
vollkommen frei bleibt).

**:::**

### Supporting TROPOMI OCAL (3) ###


_Tue, 06. January 2015 -- 16:15_

With the growing amount of data/information coming in from the OCAL measurements,
a good argument can be made in favour of keeping interfaces to that data clean and
easily usable. One thing to be considered part of this is small webpage I have been
setting up to provide an overview of the available diagnostics plots:

![Quicklooks webpage screenshot](/blog/2015/01/2015-01-06_webpage_quicklooks.png)

Pages such as the one above were quite common -- trying to cover all possible combinations
of measurement type and diagnostic plots there typically only is a subset for which
viable information exists. In order to avoid bloating the webpages, making it harder
to find what actually is available, a simple filter has been added to the Bash script
generating the intermediate files (from which [Doxygen](http://www.doxygen.org) then
generates the actual HTML pages):

~~~~ bash
    nofQuicklooks=`find_nof_files $1 $2 ${varQuicklook}`
    # only create link if there is some contents to link to
    if [[ ${nofQuicklooks} -ne 0 ]] ; then
        echo " - \subpage ${varAnchor}  (${nofQuicklooks} files)"
    fi
~~~~

A very simple measure, but quite effective -- with this getting around to the actually
available diagnostics plots becomes more straight forward (and links are added the moment
the corresponding data arrive).

**:::**

### Supporting TROPOMI OCAL (2) ###


_Mon, 05. January 2015 -- 23:30_

[As mentioned before](/blog/2014/12/2014-12-29_19-34.html) the **TROPO**spheric
**M**onitoring **I**nstrument ([TROPOMI](/work/tropomi.html)) is at the [Centre Spatial de Liege](http://www.csl.ulg.ac.be)
(CSL) [testing facility](http://www.csl.ulg.ac.be/jcms/c_5574) being subjected to
a long list of intense calibration measurements. And while data are streaming back to
the participating institutes, processing and analysis need to keep up with what is
being done at the test facility.

As a small contribution to this I have been setting up a number of scripts to operate
at the back-end of the standard processing pipeline, collecting generated diagnostics
plots and making them available in a fashion more convenient than digging through
multiple levels of output directories. In a first version I have been creating a very
simple web page directly from a Bash script -- something to get off the ground quick
to help people out (here at the institute as well as on site in Belgium). But with
the growing amount of data the original approach no longer was viable -- hence I
decided to delegate the web page creation to [Doxygen](http://www.doxygen.org), shifting
the focus to providing to easier navigation and access. The new version of the web
pages (there now are multiple instead of the original single one) looks something like
this:

![Quicklooks webpage](/blog/2015/01/2015-01-05_webpage_quicklooks.png)

One of the things I added this morning is the information on a) the number of files
collected underneath a given category and b) the number of pages the PDF files with
the diagnostics plots have. If anything this gives an idea of the data volume (or
lack thereof) for given combination of measurement types an corresponding diagnostics.

The mechanics behind the scenes are fairly straight-forward (and are the bits which
were done rather quickly); first of all -- in order to make things self-contained and
not rely on a whole collection of configuration and template files -- I am writing
out a minimal configuration script for [Doxygen](http://www.doxygen.org):

~~~~~~~~
    echo "DOXYFILE_ENCODING      = UTF-8"
    echo "PROJECT_NAME           = \"${varPageTitle}\""
    echo "FULL_PATH_NAMES        = NO"
    echo "MARKDOWN_SUPPORT       = YES"
    echo "SHOW_NAMESPACES        = NO"
    echo "INPUT                  = ${varTmpDir}"
    echo "RECURSIVE              = YES"
    echo "IMAGE_PATH             = ${varSourceDirectories}"
    echo "HTML_OUTPUT            = ${varTargetDir}"
    echo "GENERATE_HTML          = YES"
    echo "GENERATE_LATEX         = NO"
    echo "GENERATE_XML           = NO"
    echo "GENERATE_RTF           = NO"
~~~~~~~~

Written as a simple Bash function -- with the output directed to a file -- the same
variables are used as for the remainder of the script. The source files for the individual
pages is broken into a header segment and the main contents; this way once more
code instructions can be kept a bit more generic, making the script modular enough that
adding a new feature with little effort.

~~~~~~~~ bash
    md_header ()
    {
        varTitle=$1
        varAnchor=$2
        varTitleLength=`echo ${varTitle} | wc -c`
        ((varTitleLength--))
        s=$(printf "%-${varTitleLength}s" "=")
        varUnderscore=`echo "${s// /=}"`

        echo "${varTitle}  {#${varAnchor}}"
        echo "${varUnderscore}"
        echo ""
    }
~~~~~~~~

[Markdown support](http://www.stack.nl/~dimitri/doxygen/manual/markdown.html) was
introduced in Doxygen version 1.8.0 and simplifies greatly the usage of the tool as
a static website generator. Instead of writing all the HTML code manually (or at least
explicitely from the script), the intermediate product is a couple of simple markup
files, which then in turn via Doxygen are converted into a rather nice-looking website.

**:::**

### Zwischenschritt automatisiert ###


_Sun, 04. January 2015 -- 20:40_

Eine sehr nuetzliche Sache: nachdem ich schon seit einer Weile einen Teil der HDR
Berechnungen im Batch-Modus laufen lasse, habe ich ueber die Feiertage endlich mal
noch das lange faellige Script geschrieben, mit welchem sich auch die Umbennenung
der urspruenglich von [Luminance HDR](http://qtpfsgui.sourceforge.net) erzeugten
HDR Dateien erledigen laesst. Fuettert man das Programm mit einer Reihe an RAW oder
TIFF Bildern, so bekommt man eine Sammlung an HDR Dateien, deren Name leider nichts
mehr mit dem Ausgangsmaterial zu tun hat, sondern einfach nur noch durchlaufend
nummeriert ist:

~~~~~~~~
    -rw-r--r--   1 lars  staff  115371820 Jan  4 20:01 hdr_1.exr
    -rw-r--r--   1 lars  staff  115662843 Jan  4 20:03 hdr_2.exr
    -rw-r--r--   1 lars  staff  116396095 Jan  4 20:05 hdr_3.exr
    -rw-r--r--   1 lars  staff  116093543 Jan  4 20:06 hdr_4.exr
~~~~~~~~

Was ich aber gerne moechte ist, dass die HDR Datei -- wie dies auch beim Rohmaterial
der Fall ist -- die Datumsinformation traegt, so dass es einfach ist Dinge zu sortieren
und wieder zu finden. Von allen Arbeitsschritten war dies bisher derjenige, welcher
mich wahrscheinlich am meisten Zeit gekostet hat (sieht man einmal von der Beschriftung
der Bilder ab, welche sich aber eben nicht automatisieren laesst). Damit ist nun aber
gluecklicherweise Schluss, denn mit ein paar Zeilen [Bash](http://www.gnu.org/software/bash)
kann man auch diese Aufgaben dem Rechner ueberlassen.

~~~~~~~~
          36 input images found.
    0   0   dsc_20150103_170548.TIF  <=  HDR 1
    1   1   dsc_20150103_170549.TIF
    2   2   dsc_20150103_170551.TIF
    3   3   dsc_20150103_170553.TIF
    4   4   dsc_20150103_170556.TIF
    5   5   dsc_20150103_170559.TIF
    6   6   dsc_20150103_170605.TIF
    7   7   dsc_20150103_170616.TIF
    8   8   dsc_20150103_170635.TIF
    9   0   dsc_20150103_171207.TIF  <=  HDR 2
    10   1   dsc_20150103_171208.TIF
    11   2   dsc_20150103_171210.TIF
    12   3   dsc_20150103_171212.TIF
    13   4   dsc_20150103_171214.TIF
    14   5   dsc_20150103_171218.TIF
    15   6   dsc_20150103_171223.TIF
    16   7   dsc_20150103_171233.TIF
    17   8   dsc_20150103_171251.TIF
    18   0   dsc_20150103_190226.TIF  <=  HDR 3
    19   1   dsc_20150103_190228.TIF
    20   2   dsc_20150103_190230.TIF
    21   3   dsc_20150103_190233.TIF
    22   4   dsc_20150103_190237.TIF
    23   5   dsc_20150103_190245.TIF
    24   6   dsc_20150103_190259.TIF
    25   7   dsc_20150103_190325.TIF
    26   8   dsc_20150103_190359.TIF
    27   0   dsc_20150103_204801.TIF  <=  HDR 4
    28   1   dsc_20150103_204803.TIF
    29   2   dsc_20150103_204805.TIF
    30   3   dsc_20150103_204809.TIF
    31   4   dsc_20150103_204814.TIF
    32   5   dsc_20150103_204824.TIF
    33   6   dsc_20150103_204843.TIF
    34   7   dsc_20150103_204916.TIF
    35   8   dsc_20150103_204950.TIF

     - nof. input images = 36
     - nof. HDR images   = 4
~~~~~~~~

Die Kleinigkeit, welche mir jetzt noch fehlt -- und dies waere dann noch einmal ein
lohnenswertes Projekt -- ist das Kopieren der Metadaten von den Rohbildern in das HDR
(falls dies so ohne weiteres geht); bedauerlicherweise gehen diese Informationen naemlich
beim Zusammenrechnen verloren und muessen am Ende noch von Hand eingefuegt werden.
