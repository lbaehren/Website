---
title: "Blog"
in_menu: false
author: "Lars Baehren"
---

# {title:} #

**:::**

### Supporting TROPOMI OCAL (3) ###


_Tue, 06. January 2015 -- 16:15_

With the growing amount of data/information coming in from the OCAL measurements,
a good argument can be made in favour of keeping interfaces to that data clean and
easily usable. One thing to be considered part of this is small webpage I have been
setting up to provide an overview of the available diagnostics plots:

![Quicklooks webpage screenshot](/blog/2015/2015-01/2015-01-06_webpage_quicklooks.png)

Pages such as the one above were quite common -- trying to cover all possible combinations
of measurement type and diagnostic plots there typically only is a subset for which
viable information exists. In order to avoid bloating the webpages, making it harder
to find what actually is available, a simple filter has been added to the Bash script
generating the intermediate files (from which [Doxygen](http://www.doxygen.org) then
generates the actual HTML pages):

~~~~ bash
    nofQuicklooks=`find_nof_files $1 $2 ${varQuicklook}`
    # only create link if there is some contents to link to
    if [[ ${nofQuicklooks} -ne 0 ]] ; then
        echo " - \subpage ${varAnchor}  (${nofQuicklooks} files)"
    fi
~~~~

A very simple measure, but quite effective -- with this getting around to the actually
available diagnostics plots becomes more straight forward (and links are added the moment
the corresponding data arrive).

**:::**

### Supporting TROPOMI OCAL (2) ###


_Mon, 05. January 2015 -- 23:30_

[As mentioned before](/blog/2014/2014-12/2014-12-29_19-34.html) the **TROPO**spheric
**M**onitoring **I**nstrument ([TROPOMI](/work/tropomi.html)) is at the [Centre Spatial de Liege](http://www.csl.ulg.ac.be)
(CSL) [testing facility](http://www.csl.ulg.ac.be/jcms/c_5574) being subjected to
a long list of intense calibration measurements. And while data are streaming back to
the participating institutes, processing and analysis need to keep up with what is
being done at the test facility.

As a small contribution to this I have been setting up a number of scripts to operate
at the back-end of the standard processing pipeline, collecting generated diagnostics
plots and making them available in a fashion more convenient than digging through
multiple levels of output directories. In a first version I have been creating a very
simple web page directly from a Bash script -- something to get off the ground quick
to help people out (here at the institute as well as on site in Belgium). But with
the growing amount of data the original approach no longer was viable -- hence I
decided to delegate the web page creation to [Doxygen](http://www.doxygen.org), shifting
the focus to providing to easier navigation and access. The new version of the web
pages (there now are multiple instead of the original single one) looks something like
this:

![Quicklooks webpage](/blog/2015/2015-01/2015-01-05_webpage_quicklooks.png)

One of the things I added this morning is the information on a) the number of files
collected underneath a given category and b) the number of pages the PDF files with
the diagnostics plots have. If anything this gives an idea of the data volume (or
lack thereof) for given combination of measurement types an corresponding diagnostics.

The mechanics behind the scenes are fairly straight-forward (and are the bits which
were done rather quickly); first of all -- in order to make things self-contained and
not rely on a whole collection of configuration and template files -- I am writing
out a minimal configuration script for [Doxygen](http://www.doxygen.org):

~~~~~~~~
    echo "DOXYFILE_ENCODING      = UTF-8"
    echo "PROJECT_NAME           = \"${varPageTitle}\""
    echo "FULL_PATH_NAMES        = NO"
    echo "MARKDOWN_SUPPORT       = YES"
    echo "SHOW_NAMESPACES        = NO"
    echo "INPUT                  = ${varTmpDir}"
    echo "RECURSIVE              = YES"
    echo "IMAGE_PATH             = ${varSourceDirectories}"
    echo "HTML_OUTPUT            = ${varTargetDir}"
    echo "GENERATE_HTML          = YES"
    echo "GENERATE_LATEX         = NO"
    echo "GENERATE_XML           = NO"
    echo "GENERATE_RTF           = NO"
~~~~~~~~

Written as a simple Bash function -- with the output directed to a file -- the same
variables are used as for the remainder of the script. The source files for the individual
pages is broken into a header segment and the main contents; this way once more
code instructions can be kept a bit more generic, making the script modular enough that
adding a new feature with little effort.

~~~~~~~~ bash
md_header ()
{
    varTitle=$1
    varAnchor=$2
    varTitleLength=`echo ${varTitle} | wc -c`
    ((varTitleLength--))
    s=$(printf "%-${varTitleLength}s" "=")
    varUnderscore=`echo "${s// /=}"`

    echo "${varTitle}  {#${varAnchor}}"
    echo "${varUnderscore}"
    echo ""
}
~~~~~~~~

[Markdown support](http://www.stack.nl/~dimitri/doxygen/manual/markdown.html) was
introduced in Doxygen version 1.8.0 and simplifies greatly the usage of the tool as
a static website generator. Instead of writing all the HTML code manually (or at least
explicitely from the script), the intermediate product is a couple of simple markup
files, which then in turn via Doxygen are converted into a rather nice-looking website.

**:::**

### Zwischenschritt automatisiert ###


_Sun, 04. January 2015 -- 20:40_

Eine sehr nuetzliche Sache: nachdem ich schon seit einer Weile einen Teil der HDR
Berechnungen im Batch-Modus laufen lasse, habe ich ueber die Feiertage endlich mal
noch das lange faellige Script geschrieben, mit welchem sich auch die Umbennenung
der urspruenglich von [Luminance HDR](http://qtpfsgui.sourceforge.net) erzeugten
HDR Dateien erledigen laesst. Fuettert man das Programm mit einer Reihe an RAW oder
TIFF Bildern, so bekommt man eine Sammlung an HDR Dateien, deren Name leider nichts
mehr mit dem Ausgangsmaterial zu tun hat, sondern einfach nur noch durchlaufend
nummeriert ist:

~~~~~~~~
-rw-r--r--   1 lars  staff  115371820 Jan  4 20:01 hdr_1.exr
-rw-r--r--   1 lars  staff  115662843 Jan  4 20:03 hdr_2.exr
-rw-r--r--   1 lars  staff  116396095 Jan  4 20:05 hdr_3.exr
-rw-r--r--   1 lars  staff  116093543 Jan  4 20:06 hdr_4.exr
~~~~~~~~

Was ich aber gerne moechte ist, dass die HDR Datei -- wie dies auch beim Rohmaterial
der Fall ist -- die Datumsinformation traegt, so dass es einfach ist Dinge zu sortieren
und wieder zu finden. Von allen Arbeitsschritten war dies bisher derjenige, welcher
mich wahrscheinlich am meisten Zeit gekostet hat (sieht man einmal von der Beschriftung
der Bilder ab, welche sich aber eben nicht automatisieren laesst). Damit ist nun aber
gluecklicherweise Schluss, denn mit ein paar Zeilen [Bash](http://www.gnu.org/software/bash)
kann man auch diese Aufgaben dem Rechner ueberlassen.

~~~~~~~~
      36 input images found.
0   0   dsc_20150103_170548.TIF  <=  HDR 1
1   1   dsc_20150103_170549.TIF
2   2   dsc_20150103_170551.TIF
3   3   dsc_20150103_170553.TIF
4   4   dsc_20150103_170556.TIF
5   5   dsc_20150103_170559.TIF
6   6   dsc_20150103_170605.TIF
7   7   dsc_20150103_170616.TIF
8   8   dsc_20150103_170635.TIF
9   0   dsc_20150103_171207.TIF  <=  HDR 2
10   1   dsc_20150103_171208.TIF
11   2   dsc_20150103_171210.TIF
12   3   dsc_20150103_171212.TIF
13   4   dsc_20150103_171214.TIF
14   5   dsc_20150103_171218.TIF
15   6   dsc_20150103_171223.TIF
16   7   dsc_20150103_171233.TIF
17   8   dsc_20150103_171251.TIF
18   0   dsc_20150103_190226.TIF  <=  HDR 3
19   1   dsc_20150103_190228.TIF
20   2   dsc_20150103_190230.TIF
21   3   dsc_20150103_190233.TIF
22   4   dsc_20150103_190237.TIF
23   5   dsc_20150103_190245.TIF
24   6   dsc_20150103_190259.TIF
25   7   dsc_20150103_190325.TIF
26   8   dsc_20150103_190359.TIF
27   0   dsc_20150103_204801.TIF  <=  HDR 4
28   1   dsc_20150103_204803.TIF
29   2   dsc_20150103_204805.TIF
30   3   dsc_20150103_204809.TIF
31   4   dsc_20150103_204814.TIF
32   5   dsc_20150103_204824.TIF
33   6   dsc_20150103_204843.TIF
34   7   dsc_20150103_204916.TIF
35   8   dsc_20150103_204950.TIF

 - nof. input images = 36
 - nof. HDR images   = 4
~~~~~~~~

Die Kleinigkeit, welche mir jetzt noch fehlt -- und dies waere dann noch einmal ein
lohnenswertes Projekt -- ist das Kopieren der Metadaten von den Rohbildern in das HDR
(falls dies so ohne weiteres geht); bedauerlicherweise gehen diese Informationen naemlich
beim Zusammenrechnen verloren und muessen am Ende noch von Hand eingefuegt werden.

**:::**

### Gelegenheit zum Upgrade ###


_Sun, 04. January 2015 -- 00:02_

Das passte doch sehr gut. Nachdem dies schon eine Weile auf der Liste der zu erledigenden
Dinge stand, haben heute Nachmittag/Abend endlich einmal die Umbauarbeiten an der
heimischen IT stattgefunden. Da es ja einiges gibt, was hier am hausinternen
Netz haengt, macht es durchaus Sinn sich ein wenig Gedanken ueber die Verschaltung
zu machen, vor allen Dingen um den Datenverkehr in bestmoeglicher Weise zu leiten.
Um alles an Kabeln neu ziehen zu koennen musste natuerlich alles fuer eine Weile offline
gehen, so dass anschliessend dann ein Neustart noetig wurde. Da mir bei dem Synology
NAS schon seit einer Weile die Erinnerung fuer ein moegliches Systemupdate entgegenblinkte,
schien dies der perfekte Zeitpunkt zu sein, dies mal gleich noch mit zu erledigen. Folgerichtig
habe ich das Update gleich mal angeworfen, nachdem die DiskStation wieder hochgefahren
war...

![Synology DSM update](/blog/2015/2015-01/2015-01-03_Synology_DSM_update.png)

Verlief alles ohne Probleme, auch wenn ich mittlerweile so im Verzug war, dass zwei
Updates durchgefuehrt werden mussten. Der dafuer benoetigte Zeitraum stimmte aber recht
mit dem ueberein, was mir zu Beginn des Prozesses angekuendigt wurde, so dass die
eigentliche Downtime recht gering blieb (schliesslich konnte man in der Zwischenzeit ja
immer noch Daten hin und her schieben).

![Synology packages up-to-date](/blog/2015/2015-01/2015-01-03_Synology_packages.png)

An den "neuen Look" muss ich mich noch ein wenig gewoehnen: es faellt schon auf, dass
sich die ganzen Bedienelemente ein wenig an dem orientieren, was man so von Apps mobiler
Geraete gewohnt ist -- auch einem grossen Bildschirm wirkt dies nicht immer unbedingt
passend.

**:::**

### Abfallkalender in elektronischer Form ###


_Fri, 02. January 2015 -- 20:32_

Na wenn dies mal nicht ein Grund ist eine Anfrage an [Bonn Orange](https://www.bonnorange.de)
rauszuschicken: ich habe eben mal noch damit begonnen die Abfalltermine fuer das neue
Jahr in den elektronischen Kalender einzutragen und musste bei der Gelegenheit (wieder
einmal) feststellen wie vollkommen unnoetig kompliziert dies doch ist. Es gibt zwar die
Moeglichkeit sich eine [Uebersicht der Abfuhrtermine](https://www.bonnorange.de/abfuhrtermine.html)
zusammenstellen zu lassen, aber alles was es als Export-Optionen gibt, sind zwei
unterschiedlich formatierte PDFs. Dies ist zwar ganz nett, wenn man sich eine Erinnerung
an den Kuehlschrank haengen will (einfach nur ausdrucken), aber fuer den Kalender im
Handy oder Rechner ist dies alles andere als brauchbar -- erst ausdrucken und dann wieder
alles eintippen ist nun wirklich nicht das, was ich mir so unter einem zeitgemaessen
Angebot vorstelle. Folgerichtige habe ich eben mal schnell eine Mail angeschickt:

> Sehr geehrte Damen und Herren,
>
> gibt es eigentlich auch die Moeglichkeit den persoenlichen Abfallkalender auch in
> iCal oder XML Format bereitzustellen? Die Druckversion als PDF ist zwar recht nett,
> aber diese ist leider nicht fuer den Import in ein einen elektronischen Kalender
> (Handy, PC) geeignet. Da die Uebersicht eh je nach Adresse zusammengestellt wird,
> wuerde es sich hierbei lediglich um ein alternatives Download-Format handeln welches
> platform-uebergreifend lesbar ist. Ich waere mit Sicherheit nicht der einzige, der
> sich ueber diesen Service freuen wuerde.
>
> Mit freundlichen Gruessen,
>
> Lars Baehren

Ist sicherlich nicht aus der Luft gegriffen -- mal schauen, was da zurueck kommt.

**:::**

### Photo-Start ins neue Jahr ###


_Thu, 01. January 2015 -- 02:23_

So ganz wollte ich mir dies ja nicht entgehen lassen: mit dem ersten Silvester / Neujahr
hier im Haus, wollte ich doch wenigsten einmal einen kleinen Eindruck davon bekommen
(und ggf. weitervermitteln) wie sich der Jahreswechsel hier in Tannnenbusch gestaltet.
Also bin ich kurz nach Mitternacht mal zu einer kleinen Tour in die Umgebung aufgebrochen
um so vielleicht ein paar Eindrueck mitbringen zu koennen. Wie sich allerdings heraustellt
bin ich nicht so sonderlich an dem Feuerwerk selber interessiert, sondern eher an
Bildern, welche die Kombination von Licht und Feuerwerksrauch erzeugen...

![Tannenbusch nach Mitternacht](/blog/2015/2015-01/dsc_20150101_015600_Tannenbusch.jpg)

So kommt es denn auch, dass ich eher Bilder wie dem obigen nachgejagt habe -- ist leider
deutlich schwerer da etwas Gutes zu finden (vor allen Dingen wenn einem staendig Leute durch
das Blickfeld laufen) als man so denkt, aber mit ein paar brauchbaren Aufnahmen bin
ich dann doch wahrscheinlich zurueckgekommen. Ist ein wenig eine Geduldsprobe, wenn
man da im Kalten steht und darauf wartet bis die Kamera mit einer Serie von Aufnahmen
fertig ist -- da ist es doch recht angenehm wieder im Warmen zu sein.

**:::**

### Fast vorbei ###


_Wed, 31. December 2014 -- 23:03_

Noch ein paar Stunden, dann heisst es Abschied nehmen von 2014. Nachdem ich vor allen
Dingen im vorherigen Jahr recht konsequent mit dem Schreiben war, gab es vor allen Dingen
Ende Sommer bis in den Herbst hinein einen grossen Aussetzer. Ausgerechnet (wieder
einmal) zu dem Zeitpunkt wo man viel haette berichten koennen, habe ich mich selber
damit abgeschossen, indem die Bastelarbeiten an den Scripten hinter den Kulissen
von Blog und Website weitere Updates verhinderten. Dies ist nun gleucklicherweise seit
einer Weile behoben, nur das Loch in der Berichterstattung bleibt bestehen.

Vorhin waren wir noch einmal eine Runde mit Sofia unterwegs: bei den Boellern, welche
ueberall gezuendet werden, kriegt der Hund so richtig Stress, so dass es schon eine
groessere Aktion war mal eine Runde zu laufen. Zurueck zuhause scheint es sich
wieder ein wenig einzurenken, aber so gegen Mitternacht gemeinsam vor die Tuere gehen
scheint nicht unbedingt eine gute Idee zu sein. Andererseits muss man dem Hund schon
zu Gute halten, dass es in den letzten drei Wochen einiges Fortschritten gegeben hat,
so dass es kommendes Silvester sicherlich deutlich anders aussehen wird.

**:::**

### Supporting TROPOMI OCAL ###


_Mon, 29. December 2014 -- 19:34_

It is happening: after month and years of preparations the TROPOMI instrument not
only is at the [Centre Spatial de Liege](http://www.csl.ulg.ac.be) (CSL) [testing
facility](http://www.csl.ulg.ac.be/jcms/c_5574), but since a few days there is a
continuous stream of measurement data coming in!

> CSL is an active research centre in the fields of space instruments, space testing
> and technologies. CSL is highly known as a test centre but it as also important
> activities in space instrumentation and advanced technologies. For years, the CSL
> has been an “ESA-coordinated facility”. Today, the Testing Department operates five
> thermal-vacuum chambers with volumes ranging from 1 to 170 m³, two shakers (88 to
> 200 kN) and a number of specialized equipments all located in a 1,000-m² clean room
> area (ISO 5 to ISO 7).

So while there is still the need to continue working on the calibration algorithms,
there of course is the more immediate need to respond to any problems encountered
while the calibration measurements are running. Part of this support was the request
arriving this afternoon for the creation of a small webpage to provide an overview
of the available quicklook documents (basically a collection of diagnostics plots).
Nothing big, just a simple webpage with a list of files (and a link for downloading
them) -- something like this:

![Webpage screenshot](/blog/2014/2014-12/2014-12-29_quicklooks_page.png)

The pretty much straight-forward way to do this is (once more) coming up with a short
shell script to locate the PDF files... and write out some HTML code. The latter is
something I haven't been doing in quite a while: what I have been doing instead was
to run some simple markup text files through e.g. [Doxygen](http://www.doxygen.org)
or -- as is the case for this website -- use a static website generator to handle the
task. So this time it is back to the basics.

~~~~~~~~ bash
html_header ()
{
    echo "<html>"
    echo "  <head>"
    echo "    <meta name=\"author\" content=\"Lars Baehren\" />"
    echo "    <meta name=\"description\" content=\"TROPOMI SWIR quicklooks\" />"
    echo "    <meta name=\"keywords\" content=\"TROPOMI, SWIR, Quicklook, SRON\" />"
    echo "    <meta name=\"generator\" content=\"Lars Baehren\" />"
    echo "  <head>"
    echo "  <body>"
    echo ""
    echo "    <h1>TROPOMI SWIR quicklooks</h1>"
    echo ""
    echo "    <i>Page generated `date`</i>"
    echo ""
}
~~~~~~~~

Luckily nothing fancy is required, so whipping a few lines of bash script to generate
HTML output isn't a big deal. Of course there always is the risk to start overdoing it
once there first working version is out: there is always something to tweak and improve,
be it making adjustments to the layout, add navigation links, etc. However for the
time being nothing of the like is in the works, so all there is to it is to let "cron"
run the script(s) and provide the people back at SRON or CSL (Luik, BE) with the input
they need.

**:::**

### Sonntagspaziergang ###


_Sun, 28. December 2014 -- 20:35_

Was fuer ein Wetter! Nachdem eigentlich schon seit einer Weile alles unter Bergen
von Schnee begraben sein sollte, stellte sich heute Morgen bei einem Blick aus dem
Fernster nicht nur heraus, dass von einer durchgehenden weissen Schneedecke nirgends
etwas zu sehen war, sondern auch noch dass der Tag recht freundlich zu werden versprach.
Kalt zwar, aber doch recht sonnig. Folgerichtig habe ich gleich Kamera und Stativ
mit zum Sport genommen, so dass ich von Mondorf aus zurueck in Richtung Tannenbusch
laufen konnte. Habe ich auch gemacht, nur leider fiel die Route etwas anders aus, als
ich dies gehofft hatte. Auch wenn ich gleich mehrmals auf den Seiten der [Rheinfaehre Mondorf](http://www.rheinfaehre-mondorf.de)
nachgeschaut hatte, war ich doch ein wenig ueberrascht, als ich heute Mittag dann vor einem
Schild stand, welches mit mitteilte, dass zwischen dem 23. Dezember und dem 5. Januar
keinerlei Faehrverkehr geplant ist. Dies bedeutete nicht nur, dass mein anvisierter
Weg ueber den Rhein angeschnitten war, sondern dass ein laengerer Marsch zur Nordbruecke
auf dem Programm stand. Nun habe ich nicht grundsaeztlich etwas gegen die Umgebung der
Siegmuendung einzuwenden, aber ich hatte doch eigentlich eher vorgehabt [Graurheindorf](http://de.wikipedia.org/wiki/Graurheindorf)
und Buschdorf unsicher zu machen -- das werde ich mir nun zunaechst einmal fuer eine andere
Gelegenheit aufsparen muessen.

![Heinrich-Hertz-Europakolleg](/blog/2014/2014-12/hdr_20141228_153854_Heinrich-Hertz-Europakolleg.jpg)

Was mich bei der ganzen Sache dann doch wieder ein klein wenig versoehnlich stimmt ist,
dass ich trotzdem mit ein paar brauchbaren Bildern zurueck nach Hause gekommen bin.
Vor allen Dingen nach der Rheinueberquerung habe ich in Graurheindorf (dem mehr Richtung
Innenstadt gelegenen Teil) ein paar der Ecken angesteuert, welche ich mir schon eine
Weile mal etwas genauer angeschaut haben wollte: dazu gehoert u.a. der Gruendstreifen
auf der Rueckseite des [Heinrich-Hertz-Europakolleg](http://www.hhek.bonn.de) und die sich
daran anschliessenden Sportanlagen.

![Tannennbusch](/blog/2014/2014-12/hdr_20141228_163008_Tannenbusch.jpg)

Vom Timing her hat es gerade noch so gereicht, dass ich in Tannenbusch angekommen ein
paar der goldenen Sonnenstrahlen mitnehmen konnte. Zwar gab es jetzt nicht unbedingt
sonderlich viele Stellen, an welchen ich hier photographisch haette zuschlagen koennen,
aber immerhin konnte ich doch mein Portfolio fuer den Stadtteil wieder ein klein wenig
aufbessern; ich bleibe dabei nach wie vor bei meinem Vorhaben, eben ein wenig andere
Eindruecke einzufangen, als was man ansonsten bei einer Bildersuche zu dieser Gegend
geboten bekommt.

**:::**

### Aenderung der Reiseplanung ###


_Sun, 28. December 2014 -- 01:02_

Da wird es wohl nichts mit den Reiseplaenen fuer das neue Jahr. Nachdem im
Laufe des gestrigen Abends immer mehr die Wettermeldungen reinkamen, stellte sich
recht schnell heraus, dass es wohl keine gute Idee sein wuerde an den Reiseplaenen nach
Utrecht festzuhalten. Eigentlich war ja angedacht, dass Julia und ich Anfang kommender
Woche mit dem Auto nach Utrecht fahren, um auf diese Weise dann endlich einmal die
dort zwischengelagerten Stuehle nach Bonn zu ueberfuehren. Da setzt allerdings ein wenig
voraus, dass das Wetter mitspielt, denn sollte aus den angekuendigten Schneeverwehungen
ernst werden, dann will man unter diesen Umstaenden ganz bestimmt nicht ohne Not
irgendwo mit dem Auto unterwegs sein. Also werde ich wohl noch eine Woche hier in
Bonn dran haengen -- was eine der netten Nebenwirkungen ist -- und dann erst am
5. Januar wieder in die Niederlande verschwinden.

**:::**

### The Interview ###


_Thu, 25. December 2014 -- 12:35_

Die Reihe zweifelhaften Entscheidungen rund um [The Interview](http://www.imdb.com/title/tt2788710)
scheinen irgendwie nicht abreissen zu wollen. Nachdem sich Sony nach einigem hin
und her nun dazu entschlossen hat den Film online zu veroeffentlichen, frage ich
mich, wieso es dann immer noch eine Begrenzung auf einzelne Laender gibt. Schaut man
auf der [offiziellen Website](https://www.seetheinterview.com) fuer den Film nach,
findet man dort unter dem Punkt "Frequently Asked Questions" folgende Aussage:

> **Can I purchase this from outside the US?**
>
> While we do hope to see the release of The Interview across the globe, for the
> time being this is limited to the USA only. You can only purchase the movie with
> a US card, and can only stream it from a US IP address.

Da schreibt doch einfach nur danach, den Film auf anderen als den offiziellen Kanaelen
zu verbreiten. [Wie The Verge berichtet](http://www.theverge.com/2014/12/24/7448253/sonys-the-interview-site-accidentally-let-anyone-download)
wird dies zusaetzlich dadurch befeuert, dass es gerade auf Sony's offizieller Website
fuer digitalen Filmverleih eine Sicherheitsluecke gegeben hat:

> It's been an interesting day for Kernel, the site powering Sony's digital
> rentals of The Interview in the US. Not only did it briefly buckle under the
> pressure of people hammering its site to watch the movie, but apparently it
> didn't do a great job in securing copies of the film, either. Those who spent
> $5.99 to watch a 48-hour rental of The Interview on their browser could simply
> share the URL of the film with anyone else. Worse yet, anyone who had access to
> the link was able to save an unprotected copy locally through a super obvious
> loophole, something The Verge was able to confirm from different browsers and
> locations.

Dass auf diesem Wege erhaltene Kopien nicht alleine auf dem Rechner bleiben, wo sie
zunaechst abgespeichert wurden, sondern recht bald den Weg zu einem breiteren
internationalen Publikum finden werden, duerfte wohl wenig verwundern.

**:::**

### Weihnachtsarbeit ###


_Thu, 25. December 2014 -- 12:18_

Da passiert doch wirklich etwas. Nach ein klein wenig Bastelarbeit habe ich es rechtzeitig
vor Heiligabend geschafft noch den Cronjob ans Laufen zu bringen, mit welche nicht nur
die Quicklooks der Messungen eingesammelt, sondern anschliessend auch auf einen Rechner
bei SRON umkopiert werden.

~~~~~~~~
File statistics (11:00):
 - Measurement set : post_env
 - Total nof files : 470
 - Existing files  : 469
 - New files ..... : 1
~~~~~~~~

Auch wenn ich nicht direkt in die Geschehnisse vor Ort eingebunden bin, so kann ich
zumindest anhand der Dateinamen erkennen, dass nun wirklich die Messungen laufen, welche
laut dem rundgeschickten Zeitplan an der Reihe waeren. So 100%ig kann dies aber noch
nicht zufrieden stellen, denn schliesslich sind bisher nirgendwo die Quicklooks zu finden,
welche von unserer Seite aus erzeugt werden sollten. Da werde ich wohl ueber die kommenden
Tage noch einmal schauen was da los ist, denn schliesslich waere die Information schon
wichtig fuer die weiteren Entscheidungen.

**:::**

### Buecher photographieren ###


_Sat, 20. December 2014 -- 16:53_

Eine der Kleinigkeiten, welche ich vorhin mal noch schnell erledigt habe -- ehe das
Tageslicht schon wieder verschunden ist -- war einen Teil der Buecher abzuphotographieren,
welche sich hier in den diversen Kisten stapeln. Das sind nicht nur eigene Bestaende,
sondern vor allen Dingen Buecher welche wir entweder hier im Haus gefunden oder aus
Fremdbestaenden uebernommen haben. Alles davon behalten geht deutlich ueber die zur
Verfuegung stehenden Lagermoeglichkeiten hinaus, so dass wirklich nur das im Haus
bleibt, was auch selber gelesen wird -- der Rest laesst sich (hoffentlich) ueber eine
Kombination von Flohmarkt, eBay und Amazon unter das Volk bringen. Da es aber eben nicht
von allen Buechern ohne weiteres brauchbare Infos online zu finden gibt, ist es
typischerweise an mir, all das abzulichten, was dann online angeboten wird. Dementsprechend
habe ich mir vorhin nach dem Essen mal Kamera und Stativ geschnappt und eine kleine
Photo-Session eingelegt.

![Books](/blog/2014/2014-12/2014-12-20_Books.png)

Die Resultate laufen zumindest durch eine minimale Pipeline, weil ich (mit auch aufgrund
der Lichtverhaeltnisse) zumindest ein \[-1, 0, +1\] Bracketing geschosssen habe (will also
heissen HDR). Ich weiss zwar nicht ob dies immer wirklich noetig ist, aber zumindest
gibt es so Bildmaterial mit vernuenftigen Farben und ausreichend Detailaufloesung.
Der zweite Batch laeuft derzeit noch durch die Konvertierung, aber das ich dies seit
einer Weile einigermassen automatisiert habe, muss ich da nicht mehr die ganze Zeit am
Rechner sitzen bleiben und alles von Hand machen (und fuer die noch verbleibenden Schritte
wird bald mal ein kleines Script faellig).

**:::**

### Daten sichern ###


_Sat, 20. December 2014 -- 00:20_

Da muss ich mal schnell eine Sicherheitskopie von machen: da ich nun auch den
dritten der zum Algorithmen testen noetigen Datensaetze erzeugt habe -- was einige
Zeit in Anspruch genommen hat -- will ich schon dafuer sorgen, dass ich nun an den
zweiten Schritt gehen kann. Da macht es sicherlich Sinn nicht immer wieder die Testdaten
von neuem zu generieren, sondern einmal zu erzeugen und dann irgendwo abzulegen.

~~~~~~~~
report.detector4.nc                          100% 1440     1.4KB/s   00:00
quicklook_detector4.pdf                      100%  784     0.8KB/s   00:00
task.status.txt                              100%    8     0.0KB/s   00:00
task.log                                     100%   15KB  14.9KB/s   00:00
report.detector4.nc                          100% 1795     1.8KB/s   00:00
testdata_absirr_band8.nc                     100% 1102MB  91.8MB/s   00:12
testdata_absirr_band7.nc                     100% 1102MB  91.8MB/s   00:12
wavelength_map_detector4.nc                  100% 2016KB   2.0MB/s   00:00
task.status.txt                              100%    8     0.0KB/s   00:00
ckd_transmittance_detector4.nc               100% 4023KB   3.9MB/s   00:00
task.log                                     100%   38KB  38.5KB/s   00:00
report.detector4.nc                          100% 1440     1.4KB/s   00:00
quicklook_detector4.pdf                      100%   13MB  13.4MB/s   00:00
task.status.txt                              100%    8     0.0KB/s   00:00
task.log                                     100%   36KB  35.8KB/s   00:00
testdata_relirr_band7.nc                     100%   12GB  86.1MB/s   02:20
report.detector4.nc                          100% 1781     1.7KB/s   00:00
wavelength_map_detector4.nc                  100% 2016KB   2.0MB/s   00:00
task.status.txt                              100%    8     0.0KB/s   00:00
ckd_transmittance_detector4.nc               100% 4023KB   3.9MB/s   00:00
testdata_relirr_band8.nc                     100%   12GB  85.5MB/s   02:21
task.log                                     100%   89KB  89.3KB/s   00:00
rerun.sh                                     100%  565     0.6KB/s   00:00
trl1brb8g.lx.nc                              100%   17GB  84.8MB/s   03:25
pass-15.ocal_pre_processing.png              100%   32KB  32.0KB/s   00:00
gseDat.nc                                    100%   15KB  15.1KB/s   00:00
ckd-index.xml                                100%   12     0.0KB/s   00:00
pass-20.nominal_processing.png               100%  133KB 133.5KB/s   00:00
trl1brb7g.lx.nc                              100%   17GB  82.7MB/s   03:30
ckd-check.xml                                100%   12     0.0KB/s   00:00
task.status.txt                              100%    8     0.0KB/s   00:00
proc_table.xml                               100% 5553     5.4KB/s   00:00
joborder-000.cfg                             100% 8660     8.5KB/s   00:00
pass-10.ckd_handling.png                     100%   27KB  27.2KB/s   00:00
engDat.nc                                    100%   14MB  13.5MB/s   00:00
logfile_L01b.txt                             100%  453KB 452.7KB/s   00:00
task.log                                     100%   10KB  10.2KB/s   00:00
~~~~~~~~

Ich bin immer wieder recht angetan, wie die Transferraten zwischen den Instituten doch
im Vergleich zu dem sind, was hier zuhause zur Verfuegung steht. Gut, die Anforderungen
schon deutlich andere, aber liebaeugeln moechte man damit ja schon.

**:::**

### Kurz vor dem Start ###


_Fri, 19. December 2014 -- 21:51_

Das versprechend ja ein paar interessante Weihnachtstage zu werden. Nach all der
Vorbereitung (welche noch nicht 100%ig abgeschlossen ist) stehen ab dem Sonntag (!)
die `post_env` Messungen fuer TROPOMI an -- dies ist der letzte Check bevor es Ernst
wird uns mit der On-Ground Calibration (OCAL) losgehen kann. Fuer alle Beteiligten
heisst dies natuerlich erhoehte (Alarm-)Bereitschaft: so sind im Vorfeld schon mal
alle Telefonnummern und Skype-Accounts eingesammelt worden, um moeglichst schnell
(und ggf. rund um die Uhr) erreichbar zu sein. Fuer mich heisst dies, dass ich an den
kommenden Tagen wahrscheinlich so manches Mal auf einen Hilferuf reagieren muss (zumindest
in der anstehenden Startphase). Da ist es doch ganz gut, dass die Generierung der
Test-Datensaetze nun klappt und ich mich den Scripten zuwenden kann, welche auf dem
[KNMI](http://www.knmi.nl) CLuster und bei [SRON](http://www.sron.nl) laufen muessen.
