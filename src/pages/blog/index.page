---
title: "Blog"
in_menu: false
author: "Lars Baehren"
---

# {title:} #

**:::**

### Buecher photographieren ###


_Sat, 20. December 2014 -- 16:53_

Eine der Kleinigkeiten, welche ich vorhin mal noch schnell erledigt habe -- ehe das
Tageslicht schon wieder verschunden ist -- war einen Teil der Buecher abzuphotographieren,
welche sich hier in den diversen Kisten stapeln. Das sind nicht nur eigene Bestaende,
sondern vor allen Dingen Buecher welche wir entweder hier im Haus gefunden oder aus
Fremdbestaenden uebernommen haben. Alles davon behalten geht deutlich ueber die zur
Verfuegung stehenden Lagermoeglichkeiten hinaus, so dass wirklich nur das im Haus
bleibt, was auch selber gelesen wird -- der Rest laesst sich (hoffentlich) ueber eine
Kombination von Flohmarkt, eBay und Amazon unter das Volk bringen. Da es aber eben nicht
von allen Buechern ohne weiteres brauchbare Infos online zu finden gibt, ist es
typischerweise an mir, all das abzulichten, was dann online angeboten wird. Dementsprechend
habe ich mir vorhin nach dem Essen mal Kamera und Stativ geschnappt und eine kleine
Photo-Session eingelegt.

![Books](/blog/2014/2014-12/2014-12-20_Books.png)

Die Resultate laufen zumindest durch eine minimale Pipeline, weil ich (mit auch aufgrund
der Lichtverhaeltnisse) zumindest ein [-1, 0, +1] Bracketing geschosssen habe (will also
heissen HDR). Ich weiss zwar nicht ob dies immer wirklich noetig ist, aber zumindest
gibt es so Bildmaterial mit vernuenftigen Farben und ausreichend Detailaufloesung.
Der zweite Batch laeuft derzeit noch durch die Konvertierung, aber das ich dies seit
einer Weile einigermassen automatisiert habe, muss ich da nicht mehr die ganze Zeit am
Rechner sitzen bleiben und alles von Hand machen (und fuer die noch verbleibenden Schritte
wird bald mal ein kleines Script faellig).

**:::**

### Daten sichern ###


_Sat, 20. December 2014 -- 00:20_

Da muss ich mal schnell eine Sicherheitskopie von machen: da ich nun auch den
dritten der zum Algorithmen testen noetigen Datensaetze erzeugt habe -- was einige
Zeit in Anspruch genommen hat -- will ich schon dafuer sorgen, dass ich nun an den
zweiten Schritt gehen kann. Da macht es sicherlich Sinn nicht immer wieder die Testdaten
von neuem zu generieren, sondern einmal zu erzeugen und dann irgendwo abzulegen.

~~~~~~~~
report.detector4.nc                          100% 1440     1.4KB/s   00:00
quicklook_detector4.pdf                      100%  784     0.8KB/s   00:00
task.status.txt                              100%    8     0.0KB/s   00:00
task.log                                     100%   15KB  14.9KB/s   00:00
report.detector4.nc                          100% 1795     1.8KB/s   00:00
testdata_absirr_band8.nc                     100% 1102MB  91.8MB/s   00:12
testdata_absirr_band7.nc                     100% 1102MB  91.8MB/s   00:12
wavelength_map_detector4.nc                  100% 2016KB   2.0MB/s   00:00
task.status.txt                              100%    8     0.0KB/s   00:00
ckd_transmittance_detector4.nc               100% 4023KB   3.9MB/s   00:00
task.log                                     100%   38KB  38.5KB/s   00:00
report.detector4.nc                          100% 1440     1.4KB/s   00:00
quicklook_detector4.pdf                      100%   13MB  13.4MB/s   00:00
task.status.txt                              100%    8     0.0KB/s   00:00
task.log                                     100%   36KB  35.8KB/s   00:00
testdata_relirr_band7.nc                     100%   12GB  86.1MB/s   02:20
report.detector4.nc                          100% 1781     1.7KB/s   00:00
wavelength_map_detector4.nc                  100% 2016KB   2.0MB/s   00:00
task.status.txt                              100%    8     0.0KB/s   00:00
ckd_transmittance_detector4.nc               100% 4023KB   3.9MB/s   00:00
testdata_relirr_band8.nc                     100%   12GB  85.5MB/s   02:21
task.log                                     100%   89KB  89.3KB/s   00:00
rerun.sh                                     100%  565     0.6KB/s   00:00
trl1brb8g.lx.nc                              100%   17GB  84.8MB/s   03:25
pass-15.ocal_pre_processing.png              100%   32KB  32.0KB/s   00:00
gseDat.nc                                    100%   15KB  15.1KB/s   00:00
ckd-index.xml                                100%   12     0.0KB/s   00:00
pass-20.nominal_processing.png               100%  133KB 133.5KB/s   00:00
trl1brb7g.lx.nc                              100%   17GB  82.7MB/s   03:30
ckd-check.xml                                100%   12     0.0KB/s   00:00
task.status.txt                              100%    8     0.0KB/s   00:00
proc_table.xml                               100% 5553     5.4KB/s   00:00
joborder-000.cfg                             100% 8660     8.5KB/s   00:00
pass-10.ckd_handling.png                     100%   27KB  27.2KB/s   00:00
engDat.nc                                    100%   14MB  13.5MB/s   00:00
logfile_L01b.txt                             100%  453KB 452.7KB/s   00:00
task.log                                     100%   10KB  10.2KB/s   00:00
~~~~~~~~

Ich bin immer wieder recht angetan, wie die Transferraten zwischen den Instituten doch
im Vergleich zu dem sind, was hier zuhause zur Verfuegung steht. Gut, die Anforderungen
schon deutlich andere, aber liebaeugeln moechte man damit ja schon.

**:::**

### Kurz vor dem Start ###


_Fri, 19. December 2014 -- 21:51_

Das versprechend ja ein paar interessante Weihnachtstage zu werden. Nach all der
Vorbereitung (welche noch nicht 100%ig abgeschlossen ist) stehen ab dem Sonntag (!)
die `post_env` Messungen fuer TROPOMI an -- dies ist der letzte Check bevor es Ernst
wird uns mit der On-Ground Calibration (OCAL) losgehen kann. Fuer alle Beteiligten
heisst dies natuerlich erhoehte (Alarm-)Bereitschaft: so sind im Vorfeld schon mal
alle Telefonnummern und Skype-Accounts eingesammelt worden, um moeglichst schnell
(und ggf. rund um die Uhr) erreichbar zu sein. Fuer mich heisst dies, dass ich an den
kommenden Tagen wahrscheinlich so manches Mal auf einen Hilferuf reagieren muss (zumindest
in der anstehenden Startphase). Da ist es doch ganz gut, dass die Generierung der
Test-Datensaetze nun klappt und ich mich den Scripten zuwenden kann, welche auf dem
[KNMI](http://www.knmi.nl) CLuster und bei [SRON](http://www.sron.nl) laufen muessen.

**:::**

### Letzte Fahrt in diesem Jahr ###


_Thu, 18. December 2014 -- 18:23_

Vorhin beim Aufbrechen habe ich noch kurz daran gedacht -- wo ich mich von Paul und
Evert verabschiedet habe -- aber zwischenzeitlich war ich den Gedanken doch mehr
beim Coden... Dies ist in der Tat der letzte ICE Fahrt zwischen Utrecht und Bonn in
diesem Jahr; zumindest reisetechnisch waere dieses Jahr damit in ein paar Stunden
abgeschlossen (sieht man einmal von der einen oder anderen Strassenbahnfahrt vor Ort
ab, welche in den kommenden zwei Wochen wohl noch anstehen wird).

![ICE Reiseplan](/blog/2014/2014-12/dsc_20141218_180640_ICE_Reiseplan.jpg)

Heute ist der ICE deutlich leerer, als dies noch letzte Woche der Fall war; dies hat
natuerlich von der praktischen Seite den grossen Vorteil, dass es ein leichtes war einen
Platz an einem der Tische zu finden, was da Arbeiten deutlich einfacher macht. So habe
ich auch schon vor Arnheim das Problem mit dem Schreiben der Metadaten geloest, welches
mich die letzten Tage umgetrieben und immer wieder aufgehalten hat. Das besonders
schoene an der Sache ist, dass die nun gefundene Loesung nicht nur funktioniert, sondern
auch insgesamt mit weniger Code auskommt (welcher dann auch eleganter ist).

~~~~~~~~
    # Create instance of TestData object
    testdata = utils.lx_testdata (infile, band_nr, reftime=reftime)
~~~~~~~~

Ok, man koennte hier natuerlich einfach sagen, dass die Komplexizitaet einfach nur
an eine andere Stelle verschoben wurde, aber a) ist der nun dort zu findende Code
deutlich aufgeraeumter und b) habe ich nun eine Funktion zur Verfuegung, welche ich
immer wieder verwenden kann. Da werde ich mich noch mal daran machen muessen, die
erzeugten Testdaten auch in dem Kalibrations-Algorithmus einzulessen (denn schliesslich
war dies ja die Motivation fuer die ganze Aktion).

**:::**

### Vorweihnachtlich leer ###


_Thu, 18. December 2014 -- 14:58_

Also man merkt doch, dass es sich nicht mehr (zumindest in mancherlei Hinsicht)
normalen Arbeitstag handelt: seit dem Mittag ist es hier im Institut recht ausgestorben,
was vor allen Dingen wohl damit zu tun hat, dass es heute eine Weihnachts-/Jahresendefeier
gibt -- da sind doch einige Leute hin verschwunden, so dass wir hier nur noch mit
einer keinen Besetzung zurueckgeblieben sind, welche hier die Stellung haellt. Da aber
die TROPOMI On-Ground Kalibration ab der folgenden Wochen ansteht, gibt es wenig Grund
sich auszuruhen: so habe ich nach dem Treffen heute Morgen mit Paul und Matthijs noch
einmal eine Reihe Aufgaben auf meiner TODO-Liste stehen, welche abgearbeitet werden
wollen. Werde ich mich Sicherheits nicht alles heute noch schaffen (das war auch eh
nicht geplant), aber bis zur Abfahrt wollte ich wenigstens noch ein paar Kleinigkeiten
fertigstellen.

**:::**

### Uhrzeit ablesen beim Autofahren ist verbotene Handynutzung ###


_Wed, 17. December 2014 -- 13:39_

Dass die Benutzung mobiler Geraete gleich in mehrerlei Hinsicht eine Herausforderung
fuer den (Strassen-)Verkehr darstellt ist mir schon klar; in wie weit sich der
Deutsche Gesetzgeber da allerdings austoben muss, ist sicherlich eine andere Sache.
So bin ich vorhin via [Heise News](http://www.heise.de/newsticker/meldung/Uhrzeit-ablesen-beim-Autofahren-ist-verbotene-Handynutzung-2498870.html)
auf ein neues Gerichtsurteil aufmerksam gemacht wurden, welches es durchaus wert
ist an dieser Stelle auch noch einmal erwaehnt zu werden:

> Ein Autofahrer darf während der Fahrt nicht einmal die Uhrzeit auf seinem
> Handy ablesen. Das entschied das Pfälzische Oberlandesgericht (OLG) Zweibrücken.
> Nach Auffassung der Richter liegt darin eine sogenannte bestimmungsgemäße
> Nutzung des Handys und damit ein Verstoß gegen die Straßenverkehrsordnung, der
> die Zahlung eines Bußgeldes rechtfertige (Az.: 1 Ss 1/14).

So wie das Gericht da argumentiert mag man dies (rein formal) vielleicht vertreten
koennen, aber in letzter Konsequenz muss man dies dann wohl doch wieder in die
Schublade mit der Aufschrift "Unsinniges" einsortieren (und am liebsten vergessen, wenn
es nicht teuer werden koennte). Geht es hier eher um moegliche Ablenkungen beim
Autofahren, oder wirklich um den Gebrauch des Handy; sollte es wirklich (woran ich
ein wenig meine Zweifel habe) um eine potentielle Ablenkung gehen, wie steht es dann
um die folgenden Dinge:

 - aus einer Wasserflasche trinken;
 - auf die Armbanduhr schauen;
 - Sender am Autoradio einstellen;
 - das Navi bedienen;
 - den Ausdruck des Routenplaner zu Rate ziehen...

Ich denke mal mit einem wenig Nachdenken laesst sich da ohne Probleme noch eine
weitaus laengere Liste aufstellen. Wenn man es sich dann genauer ueberlegt finden
sich vielleicht endlich noch die guten Argumente dafuer den Fahrer komplett vom
Steuer zu entfernen (wie z.B. Google dies ja vormacht)...

**:::**

### Durchgelaufen ###


_Wed, 17. December 2014 -- 10:34_

Das sieht doch schon einmal gut aus. Nachdem ich gestern Abend ja losgezogen bin
ohne das Laptop mitnehmen, weil dieses ja ueber Nacht die Backup-Daten von der
einzelnen Festplatte auf das wieder in Betrieb genommene Drobo umkopieren sollte,
war ich doch recht erleichtert, dass dieser Vorgang wohl erfolgreich gewesen ist.

![Drobo capacity](/blog/2014/2014-12/2014-12-17_Drobo_capacity.png)

Wie sich bei einem Vergleich mit dem Screenshot im [gestrigen Eintrag](/blog/2014/2014-12/2014-12-16_21-00.html)
erkennen laesst, habe ich vor dem Absprung aus dem Institut sogar mal noch die kleinste
der vier Festplatten gegen eine etwas groessere ausgetauscht. Da ich ja gleich mit einem
ganzen Stapel an Festplatten zurueckgekommen war, habe ich nicht auf Anhieb die
Auswahl getroffen, womit ich den zur Verfuegung stehenden Speicherplatz maximiert kriege
-- das nette ist aber, dass sich dies bei einem Drobo problemlos korregieren laesst:
einfach alte Festplatte rausziehen und neue einsetzen (keine Schrauben noetig)...
fertig.

Mit dem heutigen Morgen geht das Drobo nun online und bekommt somit offiziell die
Aufgabe zugewiesen meinen _Time Machine_ Backups ein zuhause zu bieten.

**:::**

### Preparing to copy ###


_Tue, 16. December 2014 -- 21:00_

Ok, das kann mal noch eine Weile dauern, ehe es ueberhaupt so richtig losgeht:

![Finder window](/blog/2014/2014-12/2014-12-16_File_copy.png)

Heute Nachmittag habe ich hier im Institut eine groessere Sammlung an Festplatten
-- alle irgendwo zwischen 120 GB und 1 TB -- einsammeln koennen und diese zwecks
Weiterverwendung in das [Drobo](http://www.drobo.com/storage-products) gesteckt,
welches ich hier auf dem Schreibtisch stehen habe. Da ich zuletzt ja ein wenig
Schwierigkeiten mit der Festplatte hatte, auf welcher mein [Time Machine](http://support.apple.com/en-us/HT201250)
Backup liegt, hat es mich schon eine Weile gejuckt endlich einmal weg von der
Single-Disk Loesung zu kommen. Da ich das Drobo eh fuer diesen Zweck angedacht hatte,
es mir bisher aber leider immer noch an Festpatten mangelte, war dies heute natuerlich
eine willkommene Gelegenheit da endlich einmal taetig zu werden. Wie dies ja bei
(groesseren) Betrieben durchaus schon einmal ueblich ist (vor allen Dingen wenn die
ICT eine gewisse Groesse ueberschreitet), wird in regelmaessigen Abstaenden die
alte Computer-Ausruestung unter den Mitarbeitern verscherbelt -- so auch hier bei
[SRON](http://www.sron.nl). Ich hatte dem allerdings nicht sonderlich viel Beachtung
geschenkt, weil ich jetzt nicht unbedingt den Bedarf an alten Computern habe -- da
bedurfte es schon einer Rundmail...

> De verloting is weer klaar, maar ik heb nog wat oudere computers over.
> in de "oude bibliotheek" in de kelder kan je nog kijken of er nog iets voor je is.

... und einem etwas spaeteren Gespraech mit Tobias, um es bei mir klingeln zu lassen,
dass es sich ja nicht nur um komplette Rechnern, sondern auch um allerlei Bauteile
handeln kann. Also schnell mal runter und den Keller gerannt, um zu schauen was sich
dort noch so finden liess. Da ich mit meinem recht spezifischen Wunsch wohl ein wenig
ausserhalb dessen lag, was die meisten anderen haben wollten, hatte ich noch eine
groessere Kiste mit Festplatten zur Auswahl. Wirklich davon gebrauchen konnte ich
nur einen Teil -- weil ich eben nur mit SATA Platten etwas anfangen kann -- aber
ich bin doch mit einem recht ansehnlichen Stapel zurueck ins Buero gekommen. Aus diesem
Vorrat habe ich mir (erwartungsgemaess) die groessten Platten geschnappt und ins
Drobo eingebaut -- schon eine Weile her, dass ich dies zum letzten Mal gemacht habe
(seitdem die [Synology DiskStation](https://www.synology.com/en-global/products/DS2413+)
die Rolle des zentralen Datenhub uebernommen hat, hatten die Dobos bis auf weiteres
keine Aufgabe).

![Drobo disk pack](/blog/2014/2014-12/2014-12-16_Drobo_disk_pack.png)

Da ich vorhabe die einzelne Festplatte auszumustern -- oder zumindest an anderer Stelle
einzusetzen -- sollte ich erst einmal noch dafuer sorgen, dass die bisher darauf abgelegten
Daten auf das Drobo umkopiert werden. Statt dies (wie eigentlich ueblich) von der
Kommandozeile aus zu machen, habe ich die diversen Verzeichnisse ueber den Finder
durch die Gegend gezogen; waehrend die [Git](http://git-scm.com) Repositories recht
schnell kopiert waren, ist die _Time Machine_ Datenbank doch schon ein Stueck groesser.
Und waehrend ein Kommando wie `mv` oder `cp` sofort mit der Arbeit beginnt, inventarisiert
der Finder zunaechst erst einmal alles, was er dann anschliessend durch die Gegend
schiebt; alleine schon dieser erste Schritt kann mehrere Minuten dauern, bevor ueberhaupt
ein einzelnes Bit von A nach B bewegt wird. Angesichts der Tatsache, dass es gut
230 GB sind, welche via USB Verbindung durch die Gegend geschoben werden muessen...
da spiele sogar mit dem Gedanken den Rechner ausnahmsweise mal ueber Nacht im Institut
stehen zu lassen.

**:::**

### Drahtlos - kopflos ###


_Tue, 16. December 2014 -- 13:01_

Also einen entscheidenden Nachteil haben drahtlose Kopfhoerer schon: es ist ziemlich
leicht beim Verlassen des Hauses (der Wohnung) Dinge zurueck zu lassen. Erstaunlicherweise
sind davon bei mir aber nicht die Kopfhoerer selber betroffen, sondern ich neige eher
dazu das Handy nicht mitzunehmen/einzupacken. So war ich letzte Woche Montag schon ein
gutes Stueck aus dem Haus, als mit einem Male der Ton auf den Kopfhoerern zusammenbrach;
Da ih aber wusste dass alle Akkus aufgeladen waren, foerderte eine Inspektion der
Taschen schnell den Kern des Problems zu Tage. Heute Morgen dann ist es mir erst deutlich
spaeter aufgefallen (was vor allen Dingen daran lag, dass ich nicht Nachrichten gehoert
sondern in einem [TROPOMI](http://www.tropomi.eu) Dokument gelesen habe): ich stand
schon eine Weile an der Bushaltestelle, als ich bei dem Versuch nach der Urzeit zu schauen
feststellen durfte, dass wohl noch einmal ein Abstecher zurueck in die Wohnung faellig
werden wuerde. Was dies betrifft ist ein Kopfhoerer mit Kabel definitiv von Vorteil --
die Kombo zusammen mit dem Handy ist deutlich weniger leicht zu uebersehen und zu
vergessen.

**:::**

### Mit Hund unterwegs ###


_Mon, 15. December 2014 -- 22:20_

Schon suess, die Kleine! Dieses Foto habe ich nur wenige Minuten vor dem Zeitpunkt
gemacht, zu welchem Julias Yoga-Damen aus der [Atempause](https://plus.google.com/100836942898820199676/about)
gerannt kamen... und schnurgerade auf uns zukamen (und ich bin mir ziemlich sicher
bin, dass dies nichts mit mir zu tun hatte).

![Sofia](/blog/2014/2014-12/dsc_20141214_153447_Sofia.jpg)

Neben der Tatsache, dass man im Haus deutlich mehr aufpassen muss, was man so in der
Gegend herumliegen laesst -- will man es nicht verschleppt oder angekaut haben --
sind es nun wieder auch die regelmaessigen Ausfluege an die frische Luft, welche
als klarer Hinweis darauf gelten koennen, dass es wieder einen Hund im Haus gibt.
An diesem Sonntag ist es gerade einmal eine Woche, dass wir die kleine spanische
Hundedame in der Naehe von Frankfurt abgeholt haben... es ist aber schon zu erkennen,
wie sich alle Seiten aufeinander einstellen. Da sich das Wetter am Sonntag von der
sehr freundlichen Seite zeigte, war dies die (fast schon zu) perfekte Gelegenheit
fuer einen Spaziergang ueber die Felder in Richtung Roisdorf. Zumindest was einen Teil
der Strecke entspricht war dies die Tour, welche ziemlich genau eine Woche davor auch
schon gelaufen war -- da allerdings noch um mich mit Julia zu treffen und dann in
Richtung Frankfurt aufzubrechen. Diesen Sonntag war aber mehr Aufmerksamkeit fuer meine
Begleitung erforderlich, so dass die Motivsuche ein wenig in den Hintergrund treten
musste. Hat aber recht gut geklappt, so wir sogar mit ein wenig Zeitreserve bei der
Atempause angekommen sind.

**:::**

### Repair completed ###


_Mon, 15. December 2014 -- 08:00_

Das ist natuerlich eine Art von Benachrichtigung, wie ich sie gerne zu lesen bekomme:

> Dear user,
>
>
> The system has completed repairing the degraded storage space (Volume 1) with
Hot Spare disk (Disk 11).
>
>
> Sincerely,
> Synology DiskStation

Nachdem ich am letzten November-Wochenende zwei Festplatten in der [DiskStation](https://www.synology.com/en-global/products/DS2413+)
ausgetauscht habe, brauchte das System (erwartungsgemaess) eine Weile, um die Integritaet der
Daten wieder herzustellen und alles auf die neue Konfiguration abzustimmen. Da ich mir
aber vor einer Weile die Email-Benachrichtigung eingestellt hatte, kriege ich sogar
hier in Utrecht mit, dass sich zuhause abspielt (zumindest was die IT-Infrastruktur betrifft).
Dazu gehoert natuerlich auch, wenn eine der im NAS eingebauten Festplatten einen Fehler
aufweist:

> Dear user,
>
> The system is now using Hot Spare disk (Disk 11) to repair the degraded storage space (Volume 1).
>
> Sincerely,
> Synology DiskStation

Von den 12 Einschueben, welche zur Verfuegung stehen, werden lediglich 11 aktiv genutzt;
um die Verfuegbarkeit des Systems im Falle eines Festplattenfehlers sicherzustellen,
ist eine Platte als "Hot Spare" reserviert. Kommt es also zu einem Ausfall, dann kann
der Server direkt mit der Wiederherstellung der Daten beginnen, ohne dass ich dafuer
erst einmal eine neue Festplatte einbauen muss -- dies macht die ganze Sache einfach
deutlich entspannter.

**:::**

### Funktioniert wieder ###


_Thu, 11. December 2014 -- 21:49_

Das ich es je geschafft mir die Daten, welche ich zum arbeiten mitgenommen hatte,
durch einen doofen Programmierfehler zu zerschiessen (was bedeutet dass ich diese von
zuhause noch einmal uebertragen muss), konnte ich nicht wie geplant an dem Code fuer
die Generierung der Testdaten fuer meine Algorithmen arbeiten. Aus diesem Grunde habe
ich mich dann der Maschinerie hinter dem Blog zugewandt, welche einfach schon zu
lange ein paar recht wichtige Funktionalitaeten vermisst hat; dazu gehorte vor allen
Dingen der Zusammenbau der [Index-Seite fuer den Blog](/blog/index.html). Wie so
ueblich enthaelt diese Seite die letzten N Eintraege in chronologisch umgekehrter Folge
-- worauf sich dann auch die Aufgabe ableiten laesst, welche es zu loesen gibt. Einen
Teil davon hatte ich ja bereits (neu und kompakter) implementiert, aber was mir eben
noch fehlte war der letzte Schritt, naemlich aus der Liste der letzten Eintraege die
Seite zusammenzubauen. Einen nicht unerheblichen Teil davon konnte ich von der Routine
verwenden, welche fuer den Index der in Vorbereitung befindlichen Eintraege zustaendig
ist -- der wesentliche Unterschied ist allerdings dass ich nicht nur die Ueberschriften
sondern auch den vollen Text brauche.

In der Verhangenheit hatte dies mitunter zu einigen merkwuerdigen Resulaten gefuehrt,
weil ich via diverser `grep` Statements probiert habe sowohl den Header als auch andere
Metadaten herauszufiltern. Wenn dann einer der Patterns auch im Text des Blog-Eintrages
auftauchte, konnte es durchaus schon einmal passieren, dass vereinzelte Passagen
verschluckt wurden. Damit duerfte nun aber Schluss sein -- und dies mit einem
Bruchteil an Code!

Mit zu verdanken habe ich dies dem [Advanced Bash-Scripting Guide](http://www.tldp.org/LDP/abs/html),
einer Sammlung an Anleitungen und Ausfuehrungen, welche mir schon so manches Mal
beim Scripten geholfen hat. Der Ausgangspunkt meiner Ueberlegungen war, dass ich
es via `sed` schaffen sollte einfach den kompletten Header abzuschneiden, so das
ich mir alle weiteren Verrenkungen mit `grep` sparen kann. Da mein Wissen in
Sachen "Regular Expressions" sich doch immer wieder als ein wenig lueckenhaft
herausstellt, war da der Zeitpunkt gekommen, einmal wieder den Guide zu Hand zu
nehmen. Praktischerweise findet sich darin gegen Ende ein "Sed and Awk
Micro−Primer", wo ich dann den folgenden Befehl finden konnte:

~~~~~~~~
sed 11,/^$/d
~~~~~~~~

Als Beschreibung hierzu hiess es:

> Delete from beginning of input up to, and including first blank line.

Das kam der Sache schon ziemlich nahe und lieferte auf Anhieb gleich ein Ergebnis,
welches ich verwenden konnte: einfach die entsprechende Datei via `cat` ausgeben lassen
und dann in `sed` pipen.

~~~~~~~~
cat $1 | sed 11,/^$/d
~~~~~~~~

An der Stelle ist mir dann aufgefallen, dass sich dies sogar noch ein wenig mehr verbessern
laesst, indem ich nicht nach der ersten Leerzeile, sondern nach der eh herauszufilternden
Titelzeile suche (weil diese eh anders aufbereitet werden muss). Einfach mal schnell
ausprobieren...

~~~~~~~~
cat $1 | sed 1,/"##"/d
~~~~~~~~

... und siehe da, schon habe ich ein fertiges Script, welches nir die Index-Seite fuer den
Blog produziert. Das schoene daran ist, dass dies mit deutlich weniger Code und deutlich
weniger Anfaelligkeit gegenueber dem was im Eintrag selber steht geschieht (wo es in der
Vergangenheit naemlich durchaus schon einmal Probleme gab). Damit kann also der Blog fuer
das erste **endlich** wieder online gehen!

**:::**

### Daten weg ###


_Thu, 11. December 2014 -- 18:12_

So kann man sich natuerlich auch selber sabotieren: durch die Verwechslung eines
Dateinamens habe ich mir (noch vor Arnhem) die Daten gekillt, welche ich mir noch
extra fuer die Reise auf das Laptop geschaufelt hatte. Zunaechst dachte ich, dass
ich mit den Code selber zu weit gegangen bin - zu viele Schritte ohne zwischenzeitliches
Backup -- aber nach ein wenig Suche stellte sich heraus, dass der Code (weitestgehend)
ok war... sieht man einmal von der Tatsache ab, dass ich an einer Stelle den falschen
Pfad verwendet habe. Der Effekt von dieser Aktion war, dass ich eben genau jene
Daten ueberschrieben habe, welche ich zum Testen verwenden wollte. Sieht also ganz
danach aus, als kann ich mir zumindest dieses Vorhaben fuer die Rueckfahrt abschreiben.

**:::**

### Wieder einen Schritt naeher ###


_Wed, 10. December 2014 -- 22:23_

Und doch endlich wieder einmal einen kleinen Schritt weiter. Waehrend ich in der
Zwischenzeit durchaus ein paar wenige Eintraege geschrieben habe, ist die Infrastruktur
immer noch nicht auf dem Stand, dass ich ohne weiteres den Blog mit allen dazu
gehoerenden Seiten aktualisieren kann. Was mir derzeit noch fehlt ist, dass die
Index-Seiten und die Seite mit den letzten Eintraegen aufs Neue gebaut werden; zumindest
was letzteres betrifft habe ich nun einen kleinen Fortschritt machen koennen:

~~~~~~~~
    for FILE in `ls ${PATH_BASEDIR}`
    {
        if [ -d "$FILE" ]
        then
            find $FILE -name "*.page" | grep -v index | grep -v upcoming >> ${tmpFile}
        fi
    }

    # Sort through the list of entries
    varEntries=`cat ${tmpFile} | tail -n ${nofEntries} | sort -r`
~~~~~~~~

Derzeit spuckt die Routine nur eine Liste mit den wesentlichen Eigenchaften der
Eintraege aus, aber der folgende Schritt darauf eine Seite zu basteln duerfte
(zumindest hoffe ich dies) nicht mehr so schwer sein.

Rein von der technischen Seite her bin ich mit dem bisher erstellten recht zufrieden:
was ich bis jetzt am Laufen habe ist deutlich modularer was die einzelnen Schritte
betrifft, welche es auszufuehren gibt. Dadurch dass alles als ein Bash Script geschrieben
ist, gibt es zwar nicht die gleiche Art der Modularisierung, wie die dies z.B. bei
C++ kriegen wuerde, aber da es eben nicht einfach nur ein gigantisches Script ist,
kann ich die einzelnen Teile auf immer wieder neue Weise zusammenfuegen. Dabei gibt
es einfach ein paar Standardaufgaben, welche man einmal implementieren sollte... und
dann ist es aber auch gut damit. Was jetzt aber noch ansteht ist, dass ich mir noch
einmal ein wenig Zeit dafuer reserviere, um alle Bauteile zusammenzufuegen und die
Index-Seiten auch wirklich wieder zu generieren. Wenn dies der Fall ist, steht den
regelmaessigen Updates nichts mehr im Weg.

**:::**

### Listing blog entries ###


_Thu, 04. December 2014 -- 19:56_

While some of the machinery is not back online again, I am making some progress
on streamlining the tools supposed to be working behind the scenes. Given the fact
that the current routine to list the contents of a directory (in terms of available
blog entries) now already is an improvement w.r.t. the older version, generalising
it a little it further only seemed like a logical step. The first iteration specifically
had been written with the idea in mind to list upcoming entries (and offer them
for publication); however extraction of further details was pretty generic, such
that the only thing left to adjust was the path w.r.t. which the listing was extracted.

~~~~~~~~
list_entries ()
{
    if [ -z $1 ] ; then
        varDirectory=${PATH_UPCOMING}
    else
        varDirectory=$1
    fi

    cd ${PATH_BASEDIR}

    for FILE in `ls ${varDirectory} | grep -v index.page`
    {
        varTitle=`get_entry_title ${varDirectory}/${FILE}`
        varTimeheader=`get_timeheader ${varDirectory}/${FILE}`
        echo " - ${FILE} | ${varTitle} | ${varTimeheader}"
    }
}
~~~~~~~~

The default behaviour has been kept, such that if now further argument is provided,
the contents of the directory with entries in preparation is listed.
